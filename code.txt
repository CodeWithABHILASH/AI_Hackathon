# !pip install langgraph
import os
from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph,START,END 
from langchain.memory import ConversationBufferMemory 
from langchain.chains import ConversationalRetrievalChain 
from typing import Dict, List, Literal
from pydantic import BaseModel, Field
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.output_parsers import JsonOutputParser

import os
from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
load_dotenv()

llm = AzureChatOpenAI(
    openai_api_key= os.getenv("AZURE_LLMsOpenAI_API_KEY"),
    openai_api_version=os.getenv("AZURE_LLMsOpenAI_API_VERSION"),
    azure_deployment= os.getenv("AZURE_LLMsOpenAI_GPT4O_DEPLOYMENT_NAME"),
    azure_endpoint= os.getenv("AZURE_LLMsOpenAI_ENDPOINT"),
    temperature=0.2,
    top_p=0.7,
    max_tokens=4096)
llm.invoke("Hello").content

def extract_text(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()


class GraphState(BaseModel): 
    query: str = Field(default="", description="User query input") 
    flow_type: str = Field(default=None, description="Determines whether to perform comparison or Q&A") 
    document_texts: List[str] = Field(default=None, description="List of document texts to process") 
    comparison_results: Dict[str, List[str]] = Field(default=None, description="Stores commonalities and differences between documents") 
    rag_answer: str = Field(default=None, description="Stores the RAG-based answer for Q&A flow")

def graph_state() -> GraphState: 
    return GraphState()

# Create Sample Documents

document1 = "This is a sample contract. The terms include payment within 30 days and a penalty clause." 
document2 = "This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section."

document_texts = [document1, document2]

# Load VectorDB for RAG
def get_embeddings():
    embeddings=AzureOpenAIEmbeddings(
    model=os.getenv("AZURE_LLMsOpenAI_EMBEDDINGS_DEPLOYMENT_NAME"),
    azure_endpoint=os.getenv("AZURE_LLMsOpenAI_ENDPOINT"),
    api_key=os.getenv("AZURE_LLMsOpenAI_API_KEY"),
    openai_api_version=os.getenv("AZURE_LLMsOpenAI_API_VERSION"))
    return embeddings

def create_vectordb(docs: List[str]): 
    embeddings = get_embeddings()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200) 
    split_docs = text_splitter.create_documents(docs) 
    return FAISS.from_documents(split_docs, embeddings)


class router_class(BaseModel):
    classifier: Literal["compare","rag","end"]

    
def router(query: str):
    llm = AzureChatOpenAI(
        openai_api_key= os.getenv("AZURE_LLMsOpenAI_API_KEY"),
        openai_api_version=os.getenv("AZURE_LLMsOpenAI_API_VERSION"),
        azure_deployment= os.getenv("AZURE_LLMsOpenAI_GPT4O_DEPLOYMENT_NAME"),
        azure_endpoint= os.getenv("AZURE_LLMsOpenAI_ENDPOINT"),
        temperature=0.2,
        top_p=0.7,
        max_tokens=4096,
        )

    system_prompt = """
            You are an AI assistant that classifies user queries into three categories:
            - "compare" if the query involves comparing two documents.
            - "rag" if the query requires question-answering with a retrieval-based solution.
            - "end" if the user wants to quit/exit.

            Return the response in **strict JSON format**, with **only one key**: classifier.
            Do not include any explanations or extra text.
            """
    router_prompt = ChatPromptTemplate([
    ("system",system_prompt),
    ("user", "{query}")
    ])

    # structured_llm= llm.with_structured_output(router_class)
    parser = JsonOutputParser()
    chain=router_prompt | llm |parser
    response=chain.invoke({"query":query})
    return response

res=router("end")
print(res['classifier'])


# Decision Node:Prompts user for Input Determines which flow to follow
def decision_node(state: GraphState) -> GraphState: 
    query = input("Enter your query: ")
    state=state.copy(update={"query":query})
    print({state.query})
    res = router(state.query)
    flow_type=""
    print(res)
    if "compare" in res['classifier']:
        flow_type = "comparison"
    elif "end" in res['classifier']:
        flow_type = "end"
    else:
        flow_type = "rag"
    state=state.copy(update={"flow_type":flow_type})
    return state


# Comparison Node: Compares full document texts

def comparison_node(state: GraphState) -> GraphState: 
    doc1, doc2 = state.document_texts 
    common_text = set(doc1.split()) & set(doc2.split()) 
    differences = set(doc1.split()) ^ set(doc2.split()) 
    return state.copy(update={"comparison_results": {"common": list(common_text), "differences": list(differences)}})

# Q&A Node: Uses RAG with VectorDB

def qa_node(state: GraphState) -> GraphState: 
    vectordb = create_vectordb(state.document_texts) 
    memory = ConversationBufferMemory(memory_key="chat_history") 
    chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), memory=memory) 
    response = chain.run(state.query) 
    return state.copy(update={"rag_answer": response})

def end_node(state: GraphState) -> GraphState:
    print("workflow has ended")
    return state



graph = StateGraph(GraphState)

graph.add_node("decision_node", decision_node) 
graph.add_node("comparison_node", comparison_node) 
graph.add_node("qa_node", qa_node) 
graph.add_node("end_node", end_node) 

graph.add_edge("comparison_node","decision_node") # Return to decision node after comparison
graph.add_edge("qa_node","decision_node") # Return to decision node after Q&A

# Define Conditional Edges
def route_fn(state:GraphState)->str:
    return state.flow_type
    
graph.add_conditional_edges(
    "decision_node",
    route_fn,
    {
       "comparison":"comparison_node",
       "rag":"qa_node",
       "end":"end_node"
    }
)





graph.set_entry_point("decision_node") 
workflow = graph.compile()

from IPython.display import Image, display
try:
    display(Image(workflow.get_graph().draw_mermaid_png()))
except Exception as e:
    print(e)


initial_state = GraphState(document_texts=document_texts) 
# workflow.invoke(initial_state)

for event in workflow.stream(initial_state):
    for key,value in event.items():
        print(key,value)
        