{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##LangGraph Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph,START,END \n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationBufferMemory \n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationalRetrievalChain \n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Literal\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.memory'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph,START,END \n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain.chains import ConversationalRetrievalChain \n",
    "from typing import Dict, List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import JsonOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "str expected, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m load_dotenv()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#gpt4O\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAZURE_LLMsOpenAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_LLMsOpenAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_LLMsOpenAI_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_LLMsOpenAI_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_LLMsOpenAI_API_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_LLMsOpenAI_API_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\os.py:685\u001b[0m, in \u001b[0;36m_Environ.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[0;32m    684\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)\n\u001b[1;32m--> 685\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodevalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    686\u001b[0m     putenv(key, value)\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\os.py:743\u001b[0m, in \u001b[0;36m_createenviron.<locals>.check_str\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_str\u001b[39m(value):\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 743\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr expected, not \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[1;31mTypeError\u001b[0m: str expected, not NoneType"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key= os.getenv(\"AZURE_LLMsOpenAI_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_LLMsOpenAI_API_VERSION\"),\n",
    "    azure_deployment= os.getenv(\"AZURE_LLMsOpenAI_GPT4O_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint= os.getenv(\"AZURE_LLMsOpenAI_ENDPOINT\"),\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=4096)\n",
    "llm.invoke(\"Hello\").content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Graph State\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(BaseModel): \n",
    "    query: str = Field(default=\"\", description=\"User query input\") \n",
    "    flow_type: str = Field(default=None, description=\"Determines whether to perform comparison or Q&A\") \n",
    "    document_texts: List[str] = Field(default=None, description=\"List of document texts to process\") \n",
    "    comparison_results: Dict[str, List[str]] = Field(default=None, description=\"Stores commonalities and differences between documents\") \n",
    "    rag_answer: str = Field(default=None, description=\"Stores the RAG-based answer for Q&A flow\")\n",
    "\n",
    "def graph_state() -> GraphState: \n",
    "    return GraphState()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sample Documents\n",
    "\n",
    "document1 = \"This is a sample contract. The terms include payment within 30 days and a penalty clause.\" \n",
    "document2 = \"This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.\"\n",
    "\n",
    "document_texts = [document1, document2]\n",
    "\n",
    "# Load VectorDB for RAG\n",
    "def get_embeddings():\n",
    "    embeddings=AzureOpenAIEmbeddings(\n",
    "    model=os.getenv(\"AZURE_LLMsOpenAI_EMBEDDINGS_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_LLMsOpenAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_LLMsOpenAI_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_LLMsOpenAI_API_VERSION\"))\n",
    "    return embeddings\n",
    "\n",
    "def create_vectordb(docs: List[str]): \n",
    "    embeddings = get_embeddings()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200) \n",
    "    split_docs = text_splitter.create_documents(docs) \n",
    "    return FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "class router_class(BaseModel):\n",
    "    classifier: Literal[\"compare\",\"rag\",\"end\"]\n",
    "\n",
    "    \n",
    "def router(query: str):\n",
    "    llm = AzureChatOpenAI(\n",
    "        openai_api_key= os.getenv(\"AZURE_LLMsOpenAI_API_KEY\"),\n",
    "        openai_api_version=os.getenv(\"AZURE_LLMsOpenAI_API_VERSION\"),\n",
    "        azure_deployment= os.getenv(\"AZURE_LLMsOpenAI_GPT4O_DEPLOYMENT_NAME\"),\n",
    "        azure_endpoint= os.getenv(\"AZURE_LLMsOpenAI_ENDPOINT\"),\n",
    "        temperature=0.2,\n",
    "        top_p=0.7,\n",
    "        max_tokens=4096,\n",
    "        )\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "            You are an AI assistant that classifies user queries into three categories:\n",
    "            - \"compare\" if the query involves comparing two documents.\n",
    "            - \"rag\" if the query requires question-answering with a retrieval-based solution.\n",
    "            - \"end\" if the user wants to quit/exit.\n",
    "\n",
    "            Return the response in **strict JSON format**, with **only one key**: classifier.\n",
    "            Do not include any explanations or extra text.\n",
    "            \"\"\"\n",
    "    router_prompt = ChatPromptTemplate([\n",
    "    (\"system\",system_prompt),\n",
    "    (\"user\", \"{query}\")\n",
    "    ])\n",
    "\n",
    "    # structured_llm= llm.with_structured_output(router_class)\n",
    "    parser = JsonOutputParser()\n",
    "    chain=router_prompt | llm |parser\n",
    "    response=chain.invoke({\"query\":query})\n",
    "    return response\n",
    "\n",
    "res=router(\"end\")\n",
    "print(res['classifier'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Node:Prompts user for Input Determines which flow to follow\n",
    "def decision_node(state: GraphState) -> GraphState: \n",
    "    query = input(\"Enter your query: \")\n",
    "    state=state.copy(update={\"query\":query})\n",
    "    print({state.query})\n",
    "    res = router(state.query)\n",
    "    flow_type=\"\"\n",
    "    print(res)\n",
    "    if \"compare\" in res['classifier']:\n",
    "        flow_type = \"comparison\"\n",
    "    elif \"end\" in res['classifier']:\n",
    "        flow_type = \"end\"\n",
    "    else:\n",
    "        flow_type = \"rag\"\n",
    "    state=state.copy(update={\"flow_type\":flow_type})\n",
    "    return state\n",
    "\n",
    "\n",
    "# Comparison Node: Compares full document texts\n",
    "\n",
    "def comparison_node(state: GraphState) -> GraphState: \n",
    "    doc1, doc2 = state.document_texts \n",
    "    common_text = set(doc1.split()) & set(doc2.split()) \n",
    "    differences = set(doc1.split()) ^ set(doc2.split()) \n",
    "    return state.copy(update={\"comparison_results\": {\"common\": list(common_text), \"differences\": list(differences)}})\n",
    "\n",
    "# Q&A Node: Uses RAG with VectorDB\n",
    "\n",
    "def qa_node(state: GraphState) -> GraphState: \n",
    "    vectordb = create_vectordb(state.document_texts) \n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\") \n",
    "    chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), memory=memory) \n",
    "    response = chain.run(state.query) \n",
    "    return state.copy(update={\"rag_answer\": response})\n",
    "\n",
    "def end_node(state: GraphState) -> GraphState:\n",
    "    print(\"workflow has ended\")\n",
    "    return state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Build the LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x21b99b49ff0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(GraphState)\n",
    "\n",
    "graph.add_node(\"decision_node\", decision_node) \n",
    "graph.add_node(\"comparison_node\", comparison_node) \n",
    "graph.add_node(\"qa_node\", qa_node) \n",
    "graph.add_node(\"end_node\", end_node) \n",
    "\n",
    "graph.add_edge(\"comparison_node\",\"decision_node\") # Return to decision node after comparison\n",
    "graph.add_edge(\"qa_node\",\"decision_node\") # Return to decision node after Q&A\n",
    "\n",
    "# Define Conditional Edges\n",
    "def route_fn(state:GraphState)->str:\n",
    "    return state.flow_type\n",
    "    \n",
    "graph.add_conditional_edges(\n",
    "    \"decision_node\",\n",
    "    route_fn,\n",
    "    {\n",
    "       \"comparison\":\"comparison_node\",\n",
    "       \"rag\":\"qa_node\",\n",
    "       \"end\":\"end_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.set_entry_point(\"decision_node\") \n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='mermaid.ink', port=443): Max retries exceeded with url: /img/JSV7aW5pdDogeydmbG93Y2hhcnQnOiB7J2N1cnZlJzogJ2xpbmVhcid9fX0lJQpncmFwaCBURDsKCV9fc3RhcnRfXyhbPHA+X19zdGFydF9fPC9wPl0pOjo6Zmlyc3QKCWRlY2lzaW9uX25vZGUoZGVjaXNpb25fbm9kZSkKCWNvbXBhcmlzb25fbm9kZShjb21wYXJpc29uX25vZGUpCglxYV9ub2RlKHFhX25vZGUpCgllbmRfbm9kZShbZW5kX25vZGVdKTo6Omxhc3QKCV9fc3RhcnRfXyAtLT4gZGVjaXNpb25fbm9kZTsKCWNvbXBhcmlzb25fbm9kZSAtLT4gZGVjaXNpb25fbm9kZTsKCXFhX25vZGUgLS0+IGRlY2lzaW9uX25vZGU7CglkZWNpc2lvbl9ub2RlIC0uICZuYnNwO2NvbXBhcmlzb24mbmJzcDsgLi0+IGNvbXBhcmlzb25fbm9kZTsKCWRlY2lzaW9uX25vZGUgLS4gJm5ic3A7cmFnJm5ic3A7IC4tPiBxYV9ub2RlOwoJZGVjaXNpb25fbm9kZSAtLiAmbmJzcDtlbmQmbmJzcDsgLi0+IGVuZF9ub2RlOwoJY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNmMmYwZmYsbGluZS1oZWlnaHQ6MS4yCgljbGFzc0RlZiBmaXJzdCBmaWxsLW9wYWNpdHk6MAoJY2xhc3NEZWYgbGFzdCBmaWxsOiNiZmI2ZmMK?type=png&bgColor=!white (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(workflow.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:4: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  state=state.copy(update={\"query\":query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hi'}\n",
      "{'classifier': 'rag'}\n",
      "decision_node {'query': 'hi', 'flow_type': 'rag', 'document_texts': ['This is a sample contract. The terms include payment within 30 days and a penalty clause.', 'This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:15: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  state=state.copy(update={\"flow_type\":flow_type})\n",
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:31: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:33: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(state.query)\n",
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:34: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  return state.copy(update={\"rag_answer\": response})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa_node {'query': 'hi', 'flow_type': 'rag', 'document_texts': ['This is a sample contract. The terms include payment within 30 days and a penalty clause.', 'This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.'], 'rag_answer': 'Hello! How can I assist you today?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:4: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  state=state.copy(update={\"query\":query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'end'}\n",
      "{'classifier': 'end'}\n",
      "decision_node {'query': 'end', 'flow_type': 'end', 'document_texts': ['This is a sample contract. The terms include payment within 30 days and a penalty clause.', 'This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.'], 'rag_answer': 'Hello! How can I assist you today?'}\n",
      "workflow has ended\n",
      "end_node {'query': 'end', 'flow_type': 'end', 'document_texts': ['This is a sample contract. The terms include payment within 30 days and a penalty clause.', 'This is a sample agreement. It states that payment should be made within 30 days and includes a penalty section.'], 'rag_answer': 'Hello! How can I assist you today?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2781419\\AppData\\Local\\Temp\\ipykernel_23104\\2012341382.py:15: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  state=state.copy(update={\"flow_type\":flow_type})\n"
     ]
    }
   ],
   "source": [
    "initial_state = GraphState(document_texts=document_texts) \n",
    "# workflow.invoke(initial_state)\n",
    "\n",
    "for event in workflow.stream(initial_state):\n",
    "    for key,value in event.items():\n",
    "        print(key,value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
